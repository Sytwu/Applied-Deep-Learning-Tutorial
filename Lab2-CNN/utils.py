"""
å·¥å…·å‡½å¼æ¨¡çµ„ (Utility Functions)
æä¾›è¨“ç·´éç¨‹ä¸­å¸¸ç”¨çš„è¼”åŠ©å‡½å¼ï¼ŒåŒ…å«éš¨æ©Ÿç¨®å­è¨­å®šã€è£ç½®åµæ¸¬ã€æ¨¡å‹å„²å­˜èˆ‡è¼‰å…¥ã€è¨“ç·´æ›²ç·šç¹ªè£½ç­‰ã€‚
"""

import torch
import numpy as np
import random
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Optional
import json


def set_seed(seed: int = 42) -> None:
    """
    è¨­å®šæ‰€æœ‰éš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å¯¦é©—å¯é‡ç¾æ€§
    
    Args:
        seed: éš¨æ©Ÿç¨®å­å€¼
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        # ä»¥ä¸‹è¨­å®šå¯èƒ½æœƒé™ä½æ•ˆèƒ½ï¼Œä½†å¯æé«˜å¯é‡ç¾æ€§
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def get_device() -> torch.device:
    """
    è‡ªå‹•åµæ¸¬å¯ç”¨çš„é‹ç®—è£ç½® (CUDA > MPS > CPU)
    
    Returns:
        torch.device: å¯ç”¨çš„é‹ç®—è£ç½®
    """
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print(f'ğŸš€ ä½¿ç”¨ CUDA è£ç½®: {torch.cuda.get_device_name(0)}')
    elif torch.backends.mps.is_available():
        device = torch.device('mps')
        print('ğŸ ä½¿ç”¨ Apple Silicon MPS è£ç½®')
    else:
        device = torch.device('cpu')
        print('ğŸ’» ä½¿ç”¨ CPU è£ç½®')
    
    return device


def save_checkpoint(
    model: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    epoch: int,
    best_metric: float,
    save_path: str,
    is_best: bool = False
) -> None:
    """
    å„²å­˜æ¨¡å‹æª¢æŸ¥é»
    
    Args:
        model: è¦å„²å­˜çš„æ¨¡å‹
        optimizer: å„ªåŒ–å™¨
        epoch: ç•¶å‰è¨“ç·´è¼ªæ•¸
        best_metric: æœ€ä½³æŒ‡æ¨™å€¼
        save_path: å„²å­˜è·¯å¾‘
        is_best: æ˜¯å¦ç‚ºæœ€ä½³æ¨¡å‹
    """
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'best_metric': best_metric,
    }
    
    # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    Path(save_path).parent.mkdir(parents=True, exist_ok=True)
    
    torch.save(checkpoint, save_path)
    
    if is_best:
        best_path = str(Path(save_path).parent / 'best_model.pth')
        torch.save(checkpoint, best_path)
        print(f'âœ… å„²å­˜æœ€ä½³æ¨¡å‹è‡³ {best_path}')


def load_checkpoint(
    model: torch.nn.Module,
    optimizer: Optional[torch.optim.Optimizer],
    checkpoint_path: str,
    device: torch.device
) -> Dict:
    """
    è¼‰å…¥æ¨¡å‹æª¢æŸ¥é»
    
    Args:
        model: è¦è¼‰å…¥æ¬Šé‡çš„æ¨¡å‹
        optimizer: å„ªåŒ–å™¨ (å¯é¸)
        checkpoint_path: æª¢æŸ¥é»è·¯å¾‘
        device: é‹ç®—è£ç½®
        
    Returns:
        åŒ…å« epoch å’Œ best_metric çš„å­—å…¸
    """
    if not Path(checkpoint_path).exists():
        raise FileNotFoundError(f'æ‰¾ä¸åˆ°æª¢æŸ¥é»æª”æ¡ˆ: {checkpoint_path}')
    
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    if optimizer is not None:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    print(f'âœ… æˆåŠŸè¼‰å…¥æª¢æŸ¥é»: Epoch {checkpoint["epoch"]}, Best Metric: {checkpoint["best_metric"]:.4f}')
    
    return {
        'epoch': checkpoint['epoch'],
        'best_metric': checkpoint['best_metric']
    }


def plot_training_curves(
    train_losses: List[float],
    val_losses: List[float],
    train_accs: List[float],
    val_accs: List[float],
    save_path: Optional[str] = None
) -> None:
    """
    ç¹ªè£½è¨“ç·´æ›²ç·š (æå¤±èˆ‡æº–ç¢ºç‡)
    
    Args:
        train_losses: è¨“ç·´æå¤±åˆ—è¡¨
        val_losses: é©—è­‰æå¤±åˆ—è¡¨
        train_accs: è¨“ç·´æº–ç¢ºç‡åˆ—è¡¨
        val_accs: é©—è­‰æº–ç¢ºç‡åˆ—è¡¨
        save_path: å„²å­˜åœ–è¡¨çš„è·¯å¾‘ (å¯é¸)
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    epochs = range(1, len(train_losses) + 1)
    
    # ç¹ªè£½æå¤±æ›²ç·š
    ax1.plot(epochs, train_losses, 'b-', label='è¨“ç·´æå¤±', linewidth=2)
    ax1.plot(epochs, val_losses, 'r-', label='é©—è­‰æå¤±', linewidth=2)
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.set_title('è¨“ç·´èˆ‡é©—è­‰æå¤±æ›²ç·š', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)
    
    # ç¹ªè£½æº–ç¢ºç‡æ›²ç·š
    ax2.plot(epochs, train_accs, 'b-', label='è¨“ç·´æº–ç¢ºç‡', linewidth=2)
    ax2.plot(epochs, val_accs, 'r-', label='é©—è­‰æº–ç¢ºç‡', linewidth=2)
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Accuracy (%)', fontsize=12)
    ax2.set_title('è¨“ç·´èˆ‡é©—è­‰æº–ç¢ºç‡æ›²ç·š', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=10)
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f'ğŸ“Š è¨“ç·´æ›²ç·šå·²å„²å­˜è‡³ {save_path}')
    
    plt.show()


def save_metrics(metrics: Dict, save_path: str) -> None:
    """
    å„²å­˜è¨“ç·´æŒ‡æ¨™è‡³ JSON æª”æ¡ˆ
    
    Args:
        metrics: åŒ…å«è¨“ç·´æŒ‡æ¨™çš„å­—å…¸
        save_path: å„²å­˜è·¯å¾‘
    """
    Path(save_path).parent.mkdir(parents=True, exist_ok=True)
    
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=4, ensure_ascii=False)
    
    print(f'ğŸ“ è¨“ç·´æŒ‡æ¨™å·²å„²å­˜è‡³ {save_path}')


class AverageMeter:
    """è¨ˆç®—ä¸¦å„²å­˜å¹³å‡å€¼èˆ‡ç•¶å‰å€¼"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0
    
    def update(self, val: float, n: int = 1):
        """
        æ›´æ–°çµ±è¨ˆå€¼
        
        Args:
            val: æ•¸å€¼
            n: æ¨£æœ¬æ•¸é‡
        """
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count
