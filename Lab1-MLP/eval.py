"""
è©•ä¼°æ¨¡çµ„ (Evaluation Module)
æä¾›æ¨¡å‹è©•ä¼°æŒ‡æ¨™èˆ‡é æ¸¬çµæœè¦–è¦ºåŒ–çš„è¼”åŠ©å‡½å¼

æ³¨æ„: æœ¬æ¨¡çµ„çš„å‡½å¼å·²å®Œæ•´å¯¦ä½œï¼Œå­¸ç”Ÿå¯ç›´æ¥ä½¿ç”¨
å­¸ç”Ÿçš„ä¸»è¦å¯¦ä½œé‡é»åœ¨ train.py çš„è¨“ç·´èˆ‡é©—è­‰è¿´åœˆ
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
from typing import Tuple, List, Optional
from pathlib import Path
from tqdm import tqdm


def calculate_accuracy(predictions: np.ndarray, labels: np.ndarray) -> float:
    """
    è¨ˆç®—åˆ†é¡æº–ç¢ºç‡
    
    Args:
        predictions (np.ndarray): é æ¸¬çš„é¡åˆ¥æ¨™ç±¤ï¼Œå½¢ç‹€ (N,)
        labels (np.ndarray): çœŸå¯¦çš„é¡åˆ¥æ¨™ç±¤ï¼Œå½¢ç‹€ (N,)
    
    Returns:
        float: æº–ç¢ºç‡ç™¾åˆ†æ¯” (0-100)
    """
    correct = (predictions == labels).sum()
    total = len(labels)
    accuracy = 100.0 * correct / total
    return accuracy


def evaluate_model(
    model: nn.Module,
    data_loader: DataLoader,
    device: torch.device,
    criterion: Optional[nn.Module] = None
) -> Tuple[float, float, np.ndarray, np.ndarray]:
    """
    åœ¨è³‡æ–™é›†ä¸Šè©•ä¼°æ¨¡å‹
    
    Args:
        model: è¦è©•ä¼°çš„æ¨¡å‹
        data_loader: è³‡æ–™é›†çš„ DataLoader
        device: é‹ç®—è£ç½® (cuda/mps/cpu)
        criterion: æå¤±å‡½æ•¸ (å¯é¸)
        
    Returns:
        tuple: (æå¤±, æº–ç¢ºç‡, æ‰€æœ‰é æ¸¬, æ‰€æœ‰æ¨™ç±¤)
    """
    model.eval()  # è¨­å®šæ¨¡å‹ç‚ºè©•ä¼°æ¨¡å¼
    
    total_loss = 0.0
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for images, labels in tqdm(data_loader, desc='è©•ä¼°ä¸­'):
            images = images.to(device)
            labels = labels.to(device)
            
            # å‰å‘å‚³æ’­
            outputs = model(images)
            
            # è¨ˆç®—æå¤±
            if criterion is not None:
                loss = criterion(outputs, labels)
                total_loss += loss.item() * images.size(0)
            
            # å–å¾—é æ¸¬
            _, predicted = torch.max(outputs, 1)
            
            # æ”¶é›†é æ¸¬èˆ‡æ¨™ç±¤
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # è½‰æ›ç‚º numpy é™£åˆ—
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    
    # è¨ˆç®—æŒ‡æ¨™
    avg_loss = total_loss / len(data_loader.dataset) if criterion else 0.0
    accuracy = calculate_accuracy(all_predictions, all_labels)
    
    return avg_loss, accuracy, all_predictions, all_labels


def plot_confusion_matrix(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    class_names: List[str],
    save_path: Optional[str] = None,
    figsize: Tuple[int, int] = (10, 8)
) -> None:
    """
    ä½¿ç”¨ seaborn ç†±åŠ›åœ–ç¹ªè£½æ··æ·†çŸ©é™£
    æ³¨æ„: åœ–è¡¨æ¨™ç±¤ä½¿ç”¨è‹±æ–‡ä»¥ç¬¦åˆåœ‹éš›æ…£ä¾‹
    
    Args:
        y_true: çœŸå¯¦æ¨™ç±¤
        y_pred: é æ¸¬æ¨™ç±¤
        class_names: é¡åˆ¥åç¨±åˆ—è¡¨ (ä¾‹å¦‚: ['0', '1', ..., '9'])
        save_path: å„²å­˜åœ–è¡¨çš„è·¯å¾‘ (å¯é¸)
        figsize: åœ–è¡¨å¤§å°
    """
    # è¨ˆç®—æ··æ·†çŸ©é™£
    cm = confusion_matrix(y_true, y_pred)
    
    # å»ºç«‹åœ–è¡¨
    plt.figure(figsize=figsize)
    
    # ç¹ªè£½ç†±åŠ›åœ–
    sns.heatmap(
        cm,
        annot=True,  # åœ¨æ ¼å­ä¸­é¡¯ç¤ºæ•¸å­—
        fmt='d',  # æ•´æ•¸æ ¼å¼
        cmap='Blues',  # é…è‰²æ–¹æ¡ˆ
        xticklabels=class_names,
        yticklabels=class_names,
        cbar_kws={'label': 'Count'}
    )
    
    # è¨­å®šæ¨™ç±¤èˆ‡æ¨™é¡Œ (ä½¿ç”¨è‹±æ–‡)
    plt.title('Confusion Matrix', fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('Predicted Label', fontsize=12)
    plt.ylabel('True Label', fontsize=12)
    plt.tight_layout()
    
    # å„²å­˜
    if save_path:
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f'ğŸ“Š æ··æ·†çŸ©é™£å·²å„²å­˜è‡³ {save_path}')
    
    plt.show()


def print_classification_report(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    class_names: List[str]
) -> None:
    """
    å°å‡ºè©³ç´°çš„åˆ†é¡å ±å‘Šï¼ŒåŒ…å« precision, recall èˆ‡ F1-score
    
    Args:
        y_true: çœŸå¯¦æ¨™ç±¤
        y_pred: é æ¸¬æ¨™ç±¤
        class_names: é¡åˆ¥åç¨±åˆ—è¡¨
    """
    print('\n' + '=' * 70)
    print('åˆ†é¡å ±å‘Š (Classification Report)')
    print('=' * 70)
    
    report = classification_report(
        y_true,
        y_pred,
        target_names=class_names,
        digits=4
    )
    print(report)
    
    print('\næŒ‡æ¨™èªªæ˜:')
    print('- Precision (ç²¾ç¢ºç‡): é æ¸¬ç‚ºæ­£ä¾‹ä¸­ï¼Œå¯¦éš›ç‚ºæ­£ä¾‹çš„æ¯”ä¾‹')
    print('- Recall (å¬å›ç‡): å¯¦éš›ç‚ºæ­£ä¾‹ä¸­ï¼Œè¢«æ­£ç¢ºé æ¸¬çš„æ¯”ä¾‹')
    print('- F1-Score: Precision èˆ‡ Recall çš„èª¿å’Œå¹³å‡æ•¸')
    print('- Support: å„é¡åˆ¥çš„æ¨£æœ¬æ•¸é‡')


def visualize_predictions(
    model: nn.Module,
    data_loader: DataLoader,
    device: torch.device,
    class_names: List[str],
    num_samples: int = 16,
    save_path: Optional[str] = None
) -> None:
    """
    è¦–è¦ºåŒ–æ¨¡å‹åœ¨æ¨£æœ¬å½±åƒä¸Šçš„é æ¸¬çµæœ
    åœ–è¡¨æ¨™é¡Œä½¿ç”¨è‹±æ–‡
    
    Args:
        model: è¨“ç·´å¥½çš„æ¨¡å‹
        data_loader: DataLoader
        device: é‹ç®—è£ç½®
        class_names: é¡åˆ¥åç¨±åˆ—è¡¨
        num_samples: è¦é¡¯ç¤ºçš„æ¨£æœ¬æ•¸é‡
        save_path: å„²å­˜åœ–è¡¨çš„è·¯å¾‘ (å¯é¸)
    """
    model.eval()
    
    # å–å¾—ä¸€å€‹æ‰¹æ¬¡çš„è³‡æ–™
    images, labels = next(iter(data_loader))
    images = images[:num_samples].to(device)
    labels = labels[:num_samples]
    
    # é€²è¡Œé æ¸¬
    with torch.no_grad():
        outputs = model(images)
        probabilities = torch.softmax(outputs, dim=1)
        confidences, predictions = torch.max(probabilities, 1)
    
    # ç§»è‡³ CPU ä»¥é€²è¡Œè¦–è¦ºåŒ–
    images = images.cpu()
    predictions = predictions.cpu().numpy()
    confidences = confidences.cpu().numpy()
    labels = labels.numpy()
    
    # ç¹ªè£½çµæœ
    rows = int(np.sqrt(num_samples))
    cols = int(np.ceil(num_samples / rows))
    
    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2.5, rows * 2.5))
    axes = axes.flatten() if num_samples > 1 else [axes]
    
    for idx in range(num_samples):
        ax = axes[idx]
        
        # é¡¯ç¤ºå½±åƒ (åæ­£è¦åŒ–)
        img = images[idx].squeeze()
        mean, std = 0.1307, 0.3081
        img = img * std + mean  # åæ­£è¦åŒ–
        img = torch.clamp(img, 0, 1)
        
        ax.imshow(img, cmap='gray')
        ax.axis('off')
        
        # è¨­å®šæ¨™é¡Œ (æ­£ç¢º: ç¶ è‰²ï¼ŒéŒ¯èª¤: ç´…è‰²)
        true_label = class_names[labels[idx]]
        pred_label = class_names[predictions[idx]]
        confidence = confidences[idx] * 100
        
        is_correct = labels[idx] == predictions[idx]
        color = 'green' if is_correct else 'red'
        
        title = f'True: {true_label}\nPred: {pred_label} ({confidence:.1f}%)'
        ax.set_title(title, fontsize=10, color=color, fontweight='bold')
    
    # éš±è—å¤šé¤˜çš„å­åœ–
    for idx in range(num_samples, len(axes)):
        axes[idx].axis('off')
    
    plt.suptitle('Model Predictions', fontsize=16, fontweight='bold', y=1.02)
    plt.tight_layout()
    
    if save_path:
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f'ğŸ–¼ï¸  é æ¸¬çµæœå·²å„²å­˜è‡³ {save_path}')
    
    plt.show()


def visualize_misclassified(
    model: nn.Module,
    data_loader: DataLoader,
    device: torch.device,
    class_names: List[str],
    num_samples: int = 16,
    save_path: Optional[str] = None
) -> None:
    """
    è¦–è¦ºåŒ–æ¨¡å‹éŒ¯èª¤åˆ†é¡çš„æ¨£æœ¬
    
    Args:
        model: è¨“ç·´å¥½çš„æ¨¡å‹
        data_loader: DataLoader
        device: é‹ç®—è£ç½®
        class_names: é¡åˆ¥åç¨±åˆ—è¡¨
        num_samples: è¦é¡¯ç¤ºçš„éŒ¯èª¤æ¨£æœ¬æ•¸é‡
        save_path: å„²å­˜åœ–è¡¨çš„è·¯å¾‘ (å¯é¸)
    """
    model.eval()
    
    misclassified_images = []
    misclassified_labels = []
    misclassified_preds = []
    misclassified_confs = []
    
    # æ”¶é›†éŒ¯èª¤åˆ†é¡çš„æ¨£æœ¬
    with torch.no_grad():
        for images, labels in data_loader:
            images = images.to(device)
            labels = labels.to(device)
            
            outputs = model(images)
            probabilities = torch.softmax(outputs, dim=1)
            confidences, predictions = torch.max(probabilities, 1)
            
            # æ‰¾å‡ºéŒ¯èª¤åˆ†é¡çš„æ¨£æœ¬
            misclassified_mask = predictions != labels
            
            if misclassified_mask.sum() > 0:
                misclassified_images.append(images[misclassified_mask].cpu())
                misclassified_labels.append(labels[misclassified_mask].cpu())
                misclassified_preds.append(predictions[misclassified_mask].cpu())
                misclassified_confs.append(confidences[misclassified_mask].cpu())
            
            # å¦‚æœå·²ç¶“æ”¶é›†è¶³å¤ çš„æ¨£æœ¬å°±åœæ­¢
            total_misclassified = sum(img.size(0) for img in misclassified_images)
            if total_misclassified >= num_samples:
                break
    
    if len(misclassified_images) == 0:
        print('ğŸ‰ æ²’æœ‰æ‰¾åˆ°éŒ¯èª¤åˆ†é¡çš„æ¨£æœ¬ï¼æ¨¡å‹è¡¨ç¾å®Œç¾ï¼')
        return
    
    # åˆä½µæ‰€æœ‰éŒ¯èª¤æ¨£æœ¬
    misclassified_images = torch.cat(misclassified_images, dim=0)[:num_samples]
    misclassified_labels = torch.cat(misclassified_labels, dim=0)[:num_samples]
    misclassified_preds = torch.cat(misclassified_preds, dim=0)[:num_samples]
    misclassified_confs = torch.cat(misclassified_confs, dim=0)[:num_samples]
    
    actual_num = misclassified_images.size(0)
    
    # ç¹ªè£½çµæœ
    rows = int(np.sqrt(actual_num))
    cols = int(np.ceil(actual_num / rows))
    
    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2.5, rows * 2.5))
    axes = axes.flatten() if actual_num > 1 else [axes]
    
    for idx in range(actual_num):
        ax = axes[idx]
        
        # é¡¯ç¤ºå½±åƒ
        img = misclassified_images[idx].squeeze()
        mean, std = 0.1307, 0.3081
        img = img * std + mean
        img = torch.clamp(img, 0, 1)
        
        ax.imshow(img, cmap='gray')
        ax.axis('off')
        
        # è¨­å®šæ¨™é¡Œï¼ˆå…¨éƒ¨ç”¨ç´…è‰²ï¼Œå› ç‚ºéƒ½æ˜¯éŒ¯èª¤ï¼‰
        true_label = class_names[misclassified_labels[idx].item()]
        pred_label = class_names[misclassified_preds[idx].item()]
        confidence = misclassified_confs[idx].item() * 100
        
        title = f'True: {true_label}\nPred: {pred_label} ({confidence:.1f}%)'
        ax.set_title(title, fontsize=10, color='red', fontweight='bold')
    
    # éš±è—å¤šé¤˜çš„å­åœ–
    for idx in range(actual_num, len(axes)):
        axes[idx].axis('off')
    
    plt.suptitle('Misclassified Samples', fontsize=16, fontweight='bold', y=1.02)
    plt.tight_layout()
    
    if save_path:
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f'ğŸ–¼ï¸  éŒ¯èª¤åˆ†é¡æ¨£æœ¬å·²å„²å­˜è‡³ {save_path}')
    
    plt.show()


if __name__ == '__main__':
    print('âœ… è©•ä¼°æ¨¡çµ„è¼‰å…¥æˆåŠŸ!')
    print('\nå¯ç”¨å‡½å¼:')
    print('- calculate_accuracy(): è¨ˆç®—åˆ†é¡æº–ç¢ºç‡')
    print('- evaluate_model(): åœ¨è³‡æ–™é›†ä¸Šè©•ä¼°æ¨¡å‹')
    print('- plot_confusion_matrix(): è¦–è¦ºåŒ–æ··æ·†çŸ©é™£')
    print('- print_classification_report(): å°å‡ºè©³ç´°æŒ‡æ¨™')
    print('- visualize_predictions(): è¦–è¦ºåŒ–æ¨¡å‹é æ¸¬')
    print('- visualize_misclassified(): è¦–è¦ºåŒ–éŒ¯èª¤åˆ†é¡æ¨£æœ¬')
    print('\nğŸ’¡ é€™äº›å‡½å¼å·²å®Œæ•´å¯¦ä½œï¼Œå¯ç›´æ¥ä½¿ç”¨ï¼')
    print('   å­¸ç”Ÿçš„ä¸»è¦å¯¦ä½œé‡é»åœ¨ train.py çš„è¨“ç·´èˆ‡é©—è­‰è¿´åœˆã€‚')
