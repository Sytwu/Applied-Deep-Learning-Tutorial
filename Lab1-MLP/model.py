"""
æ¨¡å‹æ¶æ§‹æ¨¡çµ„ (Model Architecture Module)
å®šç¾© MLP (Multi-Layer Perceptron) æ¨¡å‹ç”¨æ–¼ MNIST æ‰‹å¯«æ•¸å­—åˆ†é¡

å­¸ç”Ÿä»»å‹™:
- å¯¦ä½œ __init__ æ–¹æ³•ä¾†å»ºç«‹ç¶²è·¯å±¤
- å¯¦ä½œ forward æ–¹æ³•ä¾†å®šç¾©å‰å‘å‚³æ’­
"""

import torch
import torch.nn as nn


class MLP(nn.Module):
    """
    å¤šå±¤æ„ŸçŸ¥æ©Ÿ (Multi-Layer Perceptron) æ¨¡å‹
    
    æ¶æ§‹:
    - è¼¸å…¥å±¤: 784 ç¶­åº¦ (28x28 å±•å¹³å¾Œçš„å½±åƒ)
    - éš±è—å±¤ 1: 512 å€‹ç¥ç¶“å…ƒ
    - éš±è—å±¤ 2: 256 å€‹ç¥ç¶“å…ƒ  
    - éš±è—å±¤ 3: 128 å€‹ç¥ç¶“å…ƒ
    - è¼¸å‡ºå±¤: 10 å€‹é¡åˆ¥ (æ•¸å­— 0-9)
    
    æ¯å€‹éš±è—å±¤åŒ…å«:
    - Linear è½‰æ›
    - Batch Normalization
    - ReLU æ¿€æ´»å‡½æ•¸
    - Dropout æ­£è¦åŒ–
    """
    
    def __init__(self, input_size=784, num_classes=10, dropout_rate=0.3):
        """
        åˆå§‹åŒ– MLP æ¨¡å‹
        
        Args:
            input_size (int): è¼¸å…¥ç‰¹å¾µç¶­åº¦ (28*28=784)
            num_classes (int): è¼¸å‡ºé¡åˆ¥æ•¸é‡ (æ•¸å­— 0-9 å…± 10 é¡)
            dropout_rate (float): Dropout æ©Ÿç‡ (0.3 = 30% dropout)
        """
        super(MLP, self).__init__()
        
        # ========================================
        # TODO: å­¸ç”Ÿå¯¦ä½œå€
        # ========================================
        # è«‹å¯¦ä½œä»¥ä¸‹çµæ§‹çš„ç¥ç¶“ç¶²è·¯:
        #
        # éš±è—å±¤ 1: 784 -> 512
        #   - Linear å±¤: nn.Linear(input_size, 512)
        #   - Batch normalization: nn.BatchNorm1d(512)
        #   - æ¿€æ´»å‡½æ•¸: nn.ReLU()
        #   - Dropout: nn.Dropout(dropout_rate)
        #
        # éš±è—å±¤ 2: 512 -> 256
        #   - Linear å±¤: nn.Linear(512, 256)
        #   - Batch normalization: nn.BatchNorm1d(256)
        #   - æ¿€æ´»å‡½æ•¸: nn.ReLU()
        #   - Dropout: nn.Dropout(dropout_rate)
        #
        # éš±è—å±¤ 3: 256 -> 128
        #   - Linear å±¤: nn.Linear(256, 128)
        #   - Batch normalization: nn.BatchNorm1d(128)
        #   - æ¿€æ´»å‡½æ•¸: nn.ReLU()
        #   - Dropout: nn.Dropout(dropout_rate)
        #
        # è¼¸å‡ºå±¤: 128 -> 10
        #   - Linear å±¤: nn.Linear(128, num_classes)
        #
        # å°æç¤º:
        # - å¯ä»¥ä½¿ç”¨ nn.Sequential() å°‡å¤šå±¤çµ„åˆåœ¨ä¸€èµ·
        # - BatchNorm æœ‰åŠ©æ–¼ç©©å®šå’ŒåŠ é€Ÿè¨“ç·´
        # - ReLU æ˜¯æ¿€æ´»å‡½æ•¸: f(x) = max(0, x)
        # - Dropout æœƒåœ¨è¨“ç·´æ™‚éš¨æ©Ÿå°‡ç¥ç¶“å…ƒè¨­ç‚º 0ï¼Œé˜²æ­¢éæ“¬åˆ
        # ========================================
        
        # ç¯„ä¾‹æ¶æ§‹ (ä½ éœ€è¦å¡«å…¥ç´°ç¯€):
        self.fc1 = None  # TODO: ç¬¬ä¸€å€‹éš±è—å±¤ (784 -> 512)
        self.bn1 = None  # TODO: ç¬¬ä¸€å±¤çš„ Batch normalization
        self.relu1 = None  # TODO: ReLU æ¿€æ´»å‡½æ•¸
        self.dropout1 = None  # TODO: Dropout å±¤
        
        self.fc2 = None  # TODO: ç¬¬äºŒå€‹éš±è—å±¤ (512 -> 256)
        self.bn2 = None  # TODO: ç¬¬äºŒå±¤çš„ Batch normalization
        self.relu2 = None  # TODO: ReLU æ¿€æ´»å‡½æ•¸
        self.dropout2 = None  # TODO: Dropout å±¤
        
        self.fc3 = None  # TODO: ç¬¬ä¸‰å€‹éš±è—å±¤ (256 -> 128)
        self.bn3 = None  # TODO: ç¬¬ä¸‰å±¤çš„ Batch normalization
        self.relu3 = None  # TODO: ReLU æ¿€æ´»å‡½æ•¸
        self.dropout3 = None  # TODO: Dropout å±¤
        
        self.fc_out = None  # TODO: è¼¸å‡ºå±¤ (128 -> 10)
        
    def forward(self, x):
        """
        å‰å‘å‚³æ’­
        
        Args:
            x (torch.Tensor): è¼¸å…¥å¼µé‡ï¼Œå½¢ç‹€ç‚º (batch_size, 1, 28, 28)
        
        Returns:
            torch.Tensor: è¼¸å‡º logitsï¼Œå½¢ç‹€ç‚º (batch_size, 10)
        
        Tensor å½¢ç‹€è½‰æ›:
            è¼¸å…¥:          (batch_size, 1, 28, 28)
            å±•å¹³å¾Œ:        (batch_size, 784)
            ç¶“é fc1:      (batch_size, 512)
            ç¶“é fc2:      (batch_size, 256)
            ç¶“é fc3:      (batch_size, 128)
            è¼¸å‡º:          (batch_size, 10)
        """
        # ========================================
        # TODO: å­¸ç”Ÿå¯¦ä½œå€
        # ========================================
        # è«‹å¯¦ä½œå‰å‘å‚³æ’­ï¼ŒåŒ…å«ä»¥ä¸‹æ­¥é©Ÿ:
        #
        # æ­¥é©Ÿ 1: å±•å¹³è¼¸å…¥
        #   - è¼¸å…¥å½¢ç‹€: (batch_size, 1, 28, 28)
        #   - è¼¸å‡ºå½¢ç‹€: (batch_size, 784)
        #   - æç¤º: ä½¿ç”¨ x.view(batch_size, -1) æˆ– x.flatten(1)
        #
        # æ­¥é©Ÿ 2: é€šééš±è—å±¤ 1
        #   - ä¾åºå¥—ç”¨ fc1, bn1, relu1, dropout1
        #   - å½¢ç‹€: (batch_size, 784) -> (batch_size, 512)
        #
        # æ­¥é©Ÿ 3: é€šééš±è—å±¤ 2
        #   - ä¾åºå¥—ç”¨ fc2, bn2, relu2, dropout2
        #   - å½¢ç‹€: (batch_size, 512) -> (batch_size, 256)
        #
        # æ­¥é©Ÿ 4: é€šééš±è—å±¤ 3
        #   - ä¾åºå¥—ç”¨ fc3, bn3, relu3, dropout3
        #   - å½¢ç‹€: (batch_size, 256) -> (batch_size, 128)
        #
        # æ­¥é©Ÿ 5: é€šéè¼¸å‡ºå±¤
        #   - å¥—ç”¨ fc_out
        #   - å½¢ç‹€: (batch_size, 128) -> (batch_size, 10)
        #   - æ³¨æ„: ä¸è¦åœ¨é€™è£¡å¥—ç”¨ softmax! CrossEntropyLoss æœƒè‡ªå‹•è™•ç†
        #
        # ç¯„ä¾‹çµæ§‹:
        # x = x.view(x.size(0), -1)  # å±•å¹³
        # x = self.fc1(x)
        # x = self.bn1(x)
        # x = self.relu1(x)
        # x = self.dropout1(x)
        # ... ç¹¼çºŒå…¶ä»–å±¤ ...
        # ========================================
        
        raise NotImplementedError("å­¸ç”Ÿéœ€è¦å¯¦ä½œå‰å‘å‚³æ’­")


def create_mlp_model():
    """
    å»ºç«‹ MLP æ¨¡å‹å¯¦ä¾‹
    
    Returns:
        MLP: åˆå§‹åŒ–çš„ MLP æ¨¡å‹
    """
    model = MLP(input_size=784, num_classes=10, dropout_rate=0.3)
    
    # è¨ˆç®—åƒæ•¸é‡
    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f'âœ… MLP æ¨¡å‹å»ºç«‹æˆåŠŸ!')
    print(f'   æ¶æ§‹: 784 -> 512 -> 256 -> 128 -> 10')
    print(f'   ç¸½åƒæ•¸é‡: {num_params:,}')
    
    return model


if __name__ == '__main__':
    # æ¸¬è©¦æ¨¡å‹
    print('ğŸ§ª æ¸¬è©¦ MLP æ¨¡å‹...\n')
    
    try:
        model = create_mlp_model()
        
        # å»ºç«‹æ¸¬è©¦è¼¸å…¥
        batch_size = 4
        test_input = torch.randn(batch_size, 1, 28, 28)
        
        # å‰å‘å‚³æ’­
        output = model(test_input)
        
        print(f'\nğŸ“Š æ¨¡å‹æ¸¬è©¦çµæœ:')
        print(f'   è¼¸å…¥å½¢ç‹€: {test_input.shape}')
        print(f'   è¼¸å‡ºå½¢ç‹€: {output.shape}')
        print(f'   é æœŸè¼¸å‡ºå½¢ç‹€: ({batch_size}, 10)')
        
        # é©—è­‰è¼¸å‡ºå½¢ç‹€
        assert output.shape == (batch_size, 10), 'è¼¸å‡ºå½¢ç‹€ä¸æ­£ç¢º!'
        print(f'\nâœ… æ¨¡å‹æ¸¬è©¦é€šé!')
        
    except NotImplementedError:
        print('\nâš ï¸  æ¨¡å‹å°šæœªå¯¦ä½œã€‚å­¸ç”Ÿéœ€è¦å®Œæˆ TODO å€å¡Šã€‚')
